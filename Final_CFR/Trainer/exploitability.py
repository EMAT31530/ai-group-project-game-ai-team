import numpy as np
import itertools
import copy

class Exploit_Calc:
    def __init__(self):
        self.trainer = 0


    def compute_exploitability(self, trainer):
        self.trainer = trainer
        gamestate = self.trainer.gamestate
        exploitability = 0
        br_strat_map = {}

        perms = itertools.permutations(gamestate.deck, 2)
        lent = 0
        for cards in perms:
            lent += 1
            next_gamestate = copy.deepcopy(gamestate)
            next_gamestate.cards = cards
            print(cards)
            for card in cards:
                next_gamestate.deck.remove(card)
            print(next_gamestate.deck)
            self.calc_br(next_gamestate, br_strat_map, 0, 1.0)
            self.calc_br(next_gamestate, br_strat_map, 1, 1.0)
        for _,v in br_strat_map.items():
            v[:] = np.where(v == np.max(v), 1, 0)
        cfr_strategy = self.trainer.get_final_strategy()
        
        
        for cards in itertools.permutations(gamestate.deck):
            next_gamestate = copy.deepcopy(gamestate)
            next_gamestate.cards = cards
            ev_1 = self.calc_ev(next_gamestate, cfr_strategy, br_strat_map)
            ev_2 = self.calc_ev(next_gamestate, br_strat_map, cfr_strategy)
            exploitability += (1.0/lent) * (ev_1 - ev_2)
        return exploitability, br_strat_map


    def calc_br(self, gamestate, br_strat_map, br_player, prob):
        if gamestate.is_terminal():
            return -gamestate.get_rewards()
        key = gamestate.get_representation()
        player = gamestate.get_active_player_index()
        possible_actions = gamestate.get_actions()
        n_actions = len(possible_actions)

        vals = np.zeros(n_actions)
        if player == br_player:
            for ia, action in enumerate(possible_actions):
                next_gamestate = gamestate.handle_action(action)
                vals[ia] = self.calc_br(next_gamestate, br_strat_map, br_player, prob)

            br_value = max(vals)
            if key not in br_strat_map:
                br_strat_map[key] = np.array([0.0, 0.0])
            br_strat_map[key] = br_strat_map[key] + prob * np.array(vals, dtype=np.float64)
            return -br_value
        else:
            strategy = self.trainer.get_node(key, n_actions).get_average_strategy()
            for ia, action in enumerate(possible_actions):
                next_gamestate = gamestate.handle_action(action)
                vals[ia] = self.calc_br(next_gamestate, br_strat_map, br_player, prob * strategy[ia])
            return -np.dot(strategy, vals)

    def calc_ev(self, gamestate, p1_strat, p2_strat):
        if gamestate.is_terminal():
            return -gamestate.get_rewards()
        
        player = gamestate.get_active_player_index()
        key = gamestate.get_representation()
  
        if player == 0:
            strat = p1_strat[key]
        else:
            strat = p2_strat[key]

        possible_actions = gamestate.get_actions()
        n_actions = len(possible_actions)

        next_evs = np.zeros(n_actions)
        for ia, action in enumerate(possible_actions):
            next_gamestate = gamestate.handle_action(action)
            next_evs[ia] = self.calc_ev(next_gamestate, p1_strat, p2_strat)
        return -np.dot(strat, next_evs)

class Explot_Vec_Calc: #in progress
    def __init__(self, getAIkey):
        self.trainer = 0
        self.getAIkey = getAIkey #maybe something liek this


    '''def group_hands_by_key(self, hands, grouping):
        keys = {}
        for hand in hands:
            key = grouping(hand)
            if key not in keys:
                keys[key] = 0
            keys[key] += 1
        return keys.items() #[0] = keys, [1] = weightings
    ''' 

    def compute_exploitability(self, trainer):
        self.trainer = trainer
        gamestate = self.trainer.gamestate
        exploitability = 0
        br_strat_map = {}
        
        for i in range(2):
            rps = np.repeat(1.0, self.trainer.private_states)
            next_gamestate = copy.deepcopy(gamestate)
            self.calc_br(next_gamestate, br_strat_map, i, rps)

        for _,v in br_strat_map.items():
            v[:] = np.where(v == np.max(v), 1, 0)
        
        cfr_strategy = self.trainer.get_final_strategy()
        next_gamestate = copy.deepcopy(gamestate)
        ev_1 = self.calc_ev(next_gamestate, cfr_strategy, br_strat_map)
        ev_2 = self.calc_ev(next_gamestate, br_strat_map, cfr_strategy)
        exploitability += (1.0/(gamestate.private_states*2)) * (ev_1 - ev_2)

        return exploitability, br_strat_map

    def calc_br(self, gamestate, br_strat_map, br_player, prob):
        if gamestate.is_terminal():
            return self.trainer.get_utility()

        #childvals =
        if gamestate.is_chance == 'pu':
            chance_outcomes = gamestate.get_public_chanceoutcomes()
            for outcome in chance_outcomes:
                next_gamestate = gamestate.handle_action(outcome)
                childvals += self.calc_br(next_gamestate, br_strat_map, br_player, prob)
            return childvals

        player = gamestate.get_active_player_index()
        possible_actions = gamestate.get_actions()
        n_actions = len(possible_actions)
        
        #key = gamestate.get_representation()
        keys, weightings = [] #[['key1', 3],['key2', 2]]
        
        vals = np.zeros(self.train.private_states) * n_actions #chacnge the rest
        if player == br_player:
            #keys used here would use hand isomorphisms

            for ia, action in enumerate(possible_actions):
                next_gamestate = gamestate.handle_action(action)
                vals[ia] = self.calc_br(next_gamestate, br_strat_map, br_player, prob)
                #1a[n1,n2,n3,...], 2a[n1,n2,n3,...],... 

            br_values = [max(vals[:,iI]) for iI in range(len(keys))]
            for iI, key in enumerate(keys):
                if key not in br_strat_map:
                    br_strat_map[key] = np.repeat(0.0, n_actions)
                br_strat_map[key] += weightings[iI] * prob * np.array(vals[:,iI], dtype=np.float64)
            
            return br_values
        else:
            #keys used here use the relavent hand abstraction from training
            keys, weightings = trainer.group_hands_by_key(gamestate)

            f = lambda key: (self.trainer.get_node(key, n_actions)).get_average_strategy()
            strategys = list(map(f, keys)) 

            for ia, action in enumerate(possible_actions):
                next_gamestate = gamestate.handle_action(action)
                vals[ia] = self.calc_br(next_gamestate, br_strat_map, br_player, prob * strategys[:,ia]) 

            finds = np.zeros(range(len(keys)))
            for iI, strat in enumerate(strategys):
                finds[iI] = np.dot(strat, vals[:,iI]) * weightings[iI]
            return finds


    def calc_ev(self, gamestate, p1_strat, p2_strat):
        if gamestate.is_terminal():
            return self.trainer.get_utility()
        
        player = gamestate.get_active_player_index()

        #theses keys are the groupings used by our ai i think?
        #weighting is how many private hands each represents
        keys, weightings = [] 
  
        if player == 0:
            f = lambda key: p1_strat[key]
            strategys = list(map(f, keys))
        else:
            f = lambda key: p2_strat[key]
            strategys = list(map(f, keys))

        possible_actions = gamestate.get_actions()
        n_actions = len(possible_actions)

        next_evs = np.zeros(n_actions)
        for ia, action in enumerate(possible_actions):
            next_gamestate = gamestate.handle_action(action)
            next_evs[ia] = self.calc_ev(next_gamestate, p1_strat, p2_strat)

        finds = np.zeros(range(len(keys)))
        for iS, strat in enumerate(strategys):
            finds[iS] = np.dot(strat, next_evs[iS][0]) * weightings[iS]
        return finds

