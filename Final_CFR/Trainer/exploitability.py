import numpy as np
import itertools
import copy

class Exploit_Calc:
    def compute_exploitability(self, trainer):
        self.trainer = trainer
        gamestate = copy.deepcopy(self.trainer.gamestate)
        gamestate.vectorised = False
        exploitability = 0
        br_strat_map = {}

        perms = list(itertools.permutations(gamestate.deck, 2*self.trainer.rules.hand_size))
        perm_num = len(perms)
        for cards in perms:
            next_gamestate = copy.deepcopy(gamestate)
            next_gamestate.hands = [[],[]]
            for i in range(self.trainer.rules.hand_size):
                for j in range(2):
                    card = cards[(i*2)+j]
                    next_gamestate.hands[j].append(card)
                    next_gamestate.deck.remove(card)
            self.calc_br(next_gamestate, br_strat_map, 0, 1.0)
            self.calc_br(next_gamestate, br_strat_map, 1, 1.0)
        for _,v in br_strat_map.items():
            v[:] = np.where(v == np.max(v), 1, 0)
        cfr_strategy = self.trainer.get_final_strategy()
        for cards in perms:
            next_gamestate = copy.deepcopy(gamestate)
            next_gamestate.hands = [[],[]]
            for i in range(self.trainer.rules.hand_size):
                for j in range(2):
                    card = cards[i+j]
                    next_gamestate.hands[j].append(card)
                    next_gamestate.deck.remove(card)
            ev_1 = self.calc_ev(next_gamestate, cfr_strategy, br_strat_map)
            ev_2 = self.calc_ev(next_gamestate, br_strat_map, cfr_strategy)
            exploitability += (ev_1 - ev_2)
        return exploitability * (1.0/perm_num) , br_strat_map

    def calc_br(self, gamestate, br_strat_map, br_player, prob):
        if gamestate.is_terminal():
            return -self.get_utility(gamestate) #HERE
        if gamestate.is_chance():
            chance_outcomes = gamestate.get_public_chanceoutcomes()
            chance_prob = 1.0/len(chance_outcomes)
            utility = 0
            for outcome in chance_outcomes:
                next_gamestate = gamestate.handle_public_chance(outcome)
                utility += self.calc_br(next_gamestate, br_strat_map, br_player, prob)
            return utility * chance_prob
        
        player = gamestate.get_active_player_index()
        key = gamestate.get_representation(gamestate.hands[player])
        possible_actions = gamestate.get_actions()
        n_actions = len(possible_actions)

        vals = np.zeros(n_actions)
        if player == br_player:
            for ia, action in enumerate(possible_actions):
                next_gamestate = gamestate.handle_action(action)
                vals[ia] = self.calc_br(next_gamestate, br_strat_map, br_player, prob)

            br_value = max(vals)
            if key not in br_strat_map:
                br_strat_map[key] = np.zeros(n_actions)

            br_strat_map[key] = br_strat_map[key] + prob * np.array(vals, dtype=np.float64)
            return -br_value
        else:
            strategy = self.trainer.get_node(key, n_actions).get_average_strategy()
            for ia, action in enumerate(possible_actions):
                next_gamestate = gamestate.handle_action(action)
                vals[ia] = self.calc_br(next_gamestate, br_strat_map, br_player, prob * strategy[ia])
            return -np.dot(strategy, vals)

    def calc_ev(self, gamestate, p1_strat, p2_strat):
        if gamestate.is_terminal():
            return -self.get_utility(gamestate)
        if gamestate.is_chance():
            chance_outcomes = gamestate.get_public_chanceoutcomes()
            chance_prob = 1.0/len(chance_outcomes)
            next_evs  = 0
            for outcome in chance_outcomes:
                next_gamestate = gamestate.handle_public_chance(outcome)
                next_evs += self.calc_ev(next_gamestate, p1_strat, p2_strat)
            return next_evs * chance_prob
        
        player = gamestate.get_active_player_index()
        key = gamestate.get_representation(gamestate.hands[player])
  
        if player == 0:
            strat = p1_strat[key]
        else:
            strat = p2_strat[key]

        possible_actions = gamestate.get_actions()
        n_actions = len(possible_actions)

        next_evs = np.zeros(n_actions)
        for ia, action in enumerate(possible_actions):
            next_gamestate = gamestate.handle_action(action)
            next_evs[ia] = self.calc_ev(next_gamestate, p1_strat, p2_strat)
        return -np.dot(strat, next_evs)

    def get_utility(self, gamestate):
        player = gamestate.get_active_player_index() 
        payoff = gamestate.get_payoff()

        if gamestate.is_fold():
            utility = payoff
            return utility
            
        player_rank = gamestate.get_rank(gamestate.hands[player])
        opp_rank = gamestate.get_rank(gamestate.hands[1-player])
        if player_rank > opp_rank:
            return payoff
        elif player_rank < opp_rank:
            return -payoff
        else:
            return 0

    
class Exploit_Vec_Calc:
    def compute_exploitability(self, trainer):
        self.trainer = trainer
        exploitability = 0
        br_strat_map = {}

        for player in range(2):
            rps = np.repeat(1.0, self.trainer.private_states)
            next_gamestate = copy.deepcopy(self.trainer.gamestate)
            self.calc_br(next_gamestate, br_strat_map, player, rps)
        #------------------
        for _,v in br_strat_map.items():
            v[:] = np.where(v == np.max(v), 1, 0)
        cfr_strategy = self.trainer.get_final_strategy()
        next_gamestate = copy.deepcopy(self.trainer.gamestate)
        ev_1 = np.sum(self.calc_ev(next_gamestate, cfr_strategy, br_strat_map))
        ev_2 = np.sum(self.calc_ev(next_gamestate, br_strat_map, cfr_strategy))
        exploitability += (1.0/(self.trainer.private_states)) * (ev_1 - ev_2)

        return exploitability, br_strat_map

    def calc_br(self, gamestate, br_strat_map, br_player, prob):
        if gamestate.is_terminal():
            return self.trainer.fci * self.get_utility(gamestate, self.trainer.fci * prob)
        if gamestate.is_chance():
            chance_outcomes = gamestate.get_public_chanceoutcomes()
            chance_prob = 1.0/len(chance_outcomes)
            utility  = np.zeros(self.trainer.private_states)
            for outcome in chance_outcomes:
                next_gamestate = gamestate.handle_public_chance(outcome)
                utility += self.calc_br(next_gamestate, br_strat_map, br_player, prob)
            return utility * chance_prob

        player = gamestate.get_active_player_index()
        possible_actions = gamestate.get_actions()
        n_actions = len(possible_actions)
        
        keys, indicies_grouping = self.trainer.group_hands_by_key(gamestate)
        
        vals = np.zeros((n_actions, self.trainer.private_states))
        if player == br_player:
            #keys used here would use hand isomorphisms

            for ia, action in enumerate(possible_actions):
                next_gamestate = gamestate.handle_action(action)
                vals[ia] = self.calc_br(next_gamestate, br_strat_map, br_player, prob)
                #1a[n1,n2,n3,...], 2a[n1,n2,n3,...],... 

            br_values = [max(vals[:,iI]) for iI in range(self.trainer.private_states)]
            for iI, indicies in enumerate(indicies_grouping):
                key = keys[iI]
                if key not in br_strat_map:
                    br_strat_map[key] = np.repeat(0.0, n_actions)
                for index in indicies:
                    br_strat_map[key] += prob[iI] * np.array(vals[:,iI], dtype=np.float64)
            
            return br_values
        else:
            #keys used here use the relavent hand abstraction from training
            f = lambda key: self.trainer.node_map[key].get_average_strategy()
            temp_strategy_vec = list(map(f, keys)) 
            strategy_vec = np.zeros((self.trainer.private_states, n_actions))

            for iI, indicies in enumerate(indicies_grouping):
                strategy = temp_strategy_vec[iI]
                for index in indicies:
                    strategy_vec[index] = strategy

            for ia, action in enumerate(possible_actions):
                next_gamestate = gamestate.handle_action(action)
                vals[ia] = self.calc_br(next_gamestate, br_strat_map, br_player, prob * strategy_vec[:,ia]) 

            finds = np.zeros(self.trainer.private_states)
            for iI, strat in enumerate(strategy_vec):
                finds[iI] = np.dot(strat, vals[:,iI])
            return finds

    def calc_ev(self, gamestate, p1_strat, p2_strat):
        if gamestate.is_terminal():
            return self.get_ev_rewards(gamestate)
        
        if gamestate.is_chance():
            chance_outcomes = gamestate.get_public_chanceoutcomes()
            chance_prob = 1.0/len(chance_outcomes)
            next_evs  = np.zeros(self.trainer.private_states)
            for outcome in chance_outcomes:
                next_gamestate = gamestate.handle_public_chance(outcome)
                next_evs += self.calc_ev(next_gamestate, p1_strat, p2_strat)
            return next_evs * chance_prob

        player = gamestate.get_active_player_index()
        keys, indicies_grouping = self.trainer.group_hands_by_key(gamestate)

        if player == 0:
            f = lambda key: p1_strat[key]
            strategys = list(map(f, keys))
        else:
            f = lambda key: p2_strat[key]
            strategys = list(map(f, keys))

        possible_actions = gamestate.get_actions()
        n_actions = len(possible_actions)

        next_evs = np.zeros((n_actions, self.trainer.private_states))
        for ia, action in enumerate(possible_actions):
            next_gamestate = gamestate.handle_action(action)
            next_evs[ia] = self.calc_ev(next_gamestate, p1_strat, p2_strat)

        this_evs = np.zeros(self.trainer.private_states)
        for iS, indicies in enumerate(indicies_grouping):
            strat = strategys[iS]
            evs = next_evs[:,iS]
            for index in indicies:
                this_evs[index] = np.dot(strat, evs)
        return -this_evs

    def get_ev_rewards(self, gamestate):
        lookuptable = self.trainer.rules.lookuptable
        utility = np.zeros(self.trainer.private_states)
        player = gamestate.get_active_player_index() 
        payoff = gamestate.get_payoff()
        totalmatches = len(gamestate.hands)
        seencr = np.zeros(len(lookuptable))
        for index, rank in gamestate.ranks_tuple:
            hand = self.trainer.orighands[index]
            for card in hand:
                seencr[lookuptable[card]] += 1

        if gamestate.is_fold():     
            for index, rank in gamestate.ranks_tuple:
                hand = self.trainer.orighands[index]
                totalwins = totalmatches
                for card in hand:
                    totalwins -= seencr[lookuptable[card]]
                utility[index] += totalwins
            return (utility * payoff) / totalmatches

        winsum = 0
        j = 0    
        for index, rank in gamestate.ranks_tuple:
            hand = self.trainer.orighands[index]
            while gamestate.ranks_tuple[j][1] < rank: #rank of opp hand
                winsum += 1
                j += 1 
            for card in hand:
                winsum -= seencr[lookuptable[card]]
            utility[index] += winsum

        losesum = 0
        j = len(gamestate.ranks_tuple) - 1
        reversed_tuple = gamestate.ranks_tuple.copy()
        reversed_tuple.reverse()
        for index, rank in gamestate.ranks_tuple:
            hand = self.trainer.orighands[index]
            while gamestate.ranks_tuple[j][1] < rank: #rank of opp hand
                losesum += 1
                j -= 1 
            for card in hand:
                losesum -= seencr[lookuptable[card]]
            utility[index] -= losesum

        return (utility * payoff) / totalmatches