import numpy as np
import itertools
import copy

class Exploit_Calc:
    def compute_exploitability(self, trainer):
        self.trainer = trainer
        gamestate = self.trainer.gamestate
        exploitability = 0
        br_strat_map = {}

        perms = itertools.permutations(gamestate.deck, 2)
        lent = 0
        for cards in perms:
            lent += 1
            next_gamestate = copy.deepcopy(gamestate)
            next_gamestate.cards = cards
            print(cards)
            for card in cards:
                next_gamestate.deck.remove(card)
            print(next_gamestate.deck)
            self.calc_br(next_gamestate, br_strat_map, 0, 1.0)
            self.calc_br(next_gamestate, br_strat_map, 1, 1.0)
        for _,v in br_strat_map.items():
            v[:] = np.where(v == np.max(v), 1, 0)
        cfr_strategy = self.trainer.get_final_strategy()
        
        
        for cards in itertools.permutations(gamestate.deck):
            next_gamestate = copy.deepcopy(gamestate)
            next_gamestate.cards = cards
            ev_1 = self.calc_ev(next_gamestate, cfr_strategy, br_strat_map)
            ev_2 = self.calc_ev(next_gamestate, br_strat_map, cfr_strategy)
            exploitability += (1.0/lent) * (ev_1 - ev_2)
        return exploitability, br_strat_map


    def calc_br(self, gamestate, br_strat_map, br_player, prob):
        if gamestate.is_terminal():
            return -gamestate.get_rewards()
        key = gamestate.get_representation()
        player = gamestate.get_active_player_index()
        possible_actions = gamestate.get_actions()
        n_actions = len(possible_actions)

        vals = np.zeros(n_actions)
        if player == br_player:
            for ia, action in enumerate(possible_actions):
                next_gamestate = gamestate.handle_action(action)
                vals[ia] = self.calc_br(next_gamestate, br_strat_map, br_player, prob)

            br_value = max(vals)
            if key not in br_strat_map:
                br_strat_map[key] = np.array([0.0, 0.0])
            br_strat_map[key] = br_strat_map[key] + prob * np.array(vals, dtype=np.float64)
            return -br_value
        else:
            strategy = self.trainer.get_node(key, n_actions).get_average_strategy()
            for ia, action in enumerate(possible_actions):
                next_gamestate = gamestate.handle_action(action)
                vals[ia] = self.calc_br(next_gamestate, br_strat_map, br_player, prob * strategy[ia])
            return -np.dot(strategy, vals)

    def calc_ev(self, gamestate, p1_strat, p2_strat):
        if gamestate.is_terminal():
            return -gamestate.get_rewards()
        
        player = gamestate.get_active_player_index()
        key = gamestate.get_representation()
  
        if player == 0:
            strat = p1_strat[key]
        else:
            strat = p2_strat[key]

        possible_actions = gamestate.get_actions()
        n_actions = len(possible_actions)

        next_evs = np.zeros(n_actions)
        for ia, action in enumerate(possible_actions):
            next_gamestate = gamestate.handle_action(action)
            next_evs[ia] = self.calc_ev(next_gamestate, p1_strat, p2_strat)
        return -np.dot(strat, next_evs)

class Explot_Vec_Calc:
    def compute_exploitability(self, trainer):
        self.node_map = trainer.node_map
        self.get_utility = trainer.get_utility
        self.private_states = trainer.private_states
        self.fci = np.repeat(1.0/self.private_states, self.private_states)
        gamestate = trainer.gamestate
        exploitability = 0
        br_strat_map = {}
        
        for player in range(2):
            rps = np.repeat(1.0, self.private_states)
            next_gamestate = copy.deepcopy(gamestate)
            self.calc_br(next_gamestate, br_strat_map, player, rps)
        #------------------
        for _,v in br_strat_map.items():
            v[:] = np.where(v == np.max(v), 1, 0)
        
        cfr_strategy = trainer.get_final_strategy()
        next_gamestate = copy.deepcopy(gamestate)
        ev_1 = np.sum(self.calc_ev(next_gamestate, cfr_strategy, br_strat_map))
        ev_2 = np.sum(self.calc_ev(next_gamestate, br_strat_map, cfr_strategy))
        exploitability += (1.0/(self.private_states*2)) * (ev_1 - ev_2)

        return exploitability, br_strat_map

    def calc_br(self, gamestate, br_strat_map, br_player, prob):
        if gamestate.is_terminal():
            return self.fci * self.get_utility(gamestate, self.fci * prob)

        #childvals =
        if gamestate.is_chance():
            chance_outcomes = gamestate.get_public_chanceoutcomes()
            chance_prob = 1.0/len(chance_outcomes)
            utility  = np.zeros(self.private_states)
            for outcome in chance_outcomes:
                next_gamestate = gamestate.handle_public_chance(outcome)
                utility += self.calc_br(next_gamestate, br_strat_map, br_player, prob)
            return utility * chance_prob

        player = gamestate.get_active_player_index()
        possible_actions = gamestate.get_actions()
        n_actions = len(possible_actions)
        
        keys, indicies_grouping = self.group_hands_by_key(gamestate)
        
        vals = np.zeros((n_actions, self.private_states))
        if player == br_player:
            #keys used here would use hand isomorphisms

            for ia, action in enumerate(possible_actions):
                next_gamestate = gamestate.handle_action(action)
                vals[ia] = self.calc_br(next_gamestate, br_strat_map, br_player, prob)
                #1a[n1,n2,n3,...], 2a[n1,n2,n3,...],... 

            br_values = [max(vals[:,iI]) for iI in range(self.private_states)]
            for iI, indicies in enumerate(indicies_grouping):
                key = key[iI]
                if key not in br_strat_map:
                    br_strat_map[key] = np.repeat(0.0, n_actions)
                for index in indicies:
                    br_strat_map[key] += prob[iI] * np.array(vals[:,iI], dtype=np.float64)
            
            return br_values
        else:
            #keys used here use the relavent hand abstraction from training
            f = lambda key: self.node_map[key].get_average_strategy()
            temp_strategy_vec = list(map(f, keys)) 
            strategy_vec = np.zeros((self.private_states, n_actions))

            for iI, indicies in enumerate(indicies_grouping):
                strategy = temp_strategy_vec[iI]
                for index in indicies:
                    strategy_vec[index] = strategy

            for ia, action in enumerate(possible_actions):
                next_gamestate = gamestate.handle_action(action)
                vals[ia] = self.calc_br(next_gamestate, br_strat_map, br_player, prob * strategy_vec[:,ia]) 

            finds = np.zeros(self.private_states)
            for iI, strat in enumerate(strategy_vec):
                finds[iI] = np.dot(strat, vals[:,iI])
            return finds

    def calc_ev(self, gamestate, p1_strat, p2_strat):
        if gamestate.is_terminal():
            return self.get_rewards(gamestate)
        
        if gamestate.is_chance():
            chance_outcomes = gamestate.get_public_chanceoutcomes()
            chance_prob = 1.0/len(chance_outcomes)
            next_evs  = np.zeros(self.private_states)
            for outcome in chance_outcomes:
                next_gamestate = gamestate.handle_public_chance(outcome)
                next_evs += self.calc_ev(next_gamestate, p1_strat, p2_strat)
            return next_evs * chance_prob

        player = gamestate.get_active_player_index()
        keys, indicies_grouping = self.group_hands_by_key(gamestate)

        if player == 0:
            f = lambda key: p1_strat[key]
            strategys = list(map(f, keys))
        else:
            f = lambda key: p2_strat[key]
            strategys = list(map(f, keys))

        possible_actions = gamestate.get_actions()
        n_actions = len(possible_actions)

        next_evs = np.zeros((n_actions, self.private_states))
        for ia, action in enumerate(possible_actions):
            next_gamestate = gamestate.handle_action(action)
            next_evs[ia] = self.calc_ev(next_gamestate, p1_strat, p2_strat)

        evs = np.zeros(self.private_states)
        for iS, indicies in enumerate(indicies_grouping):
            strat = strategys[iS]
            evs = next_evs[:,iS]
            for index in indicies:
                evs[index] = np.dot(strat, evs)
        return -evs

    def group_hands_by_key(self, gamestate):
        keys = {}
        for hand in gamestate.hands:
            key = gamestate.get_representation(hand[1])
            if key not in keys:
                keys[key] = []
            keys[key].append(hand[0])
        return zip(*keys.items()) 


    def get_rewards(self, gamestate):
        pass