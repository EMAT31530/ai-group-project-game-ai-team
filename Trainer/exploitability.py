import numpy as np
import itertools
import copy

class Exploit_Calc:
    def compute_exploitability(self, trainer):
        nodes_touched = trainer.nodes_touched
        self.trainer = trainer
        gamestate = copy.deepcopy(self.trainer.gamestate)
        gamestate.vectorised = False
        exploitability = 0
        br_strat_map = {}

        perms = list(itertools.permutations(gamestate.deck, 2*gamestate.rules.hand_size))
        perm_num = len(perms)
        for cards in perms:
            next_gamestate = copy.deepcopy(gamestate)
            next_gamestate.hands = [[],[]]
            for i in range(gamestate.rules.hand_size):
                for j in range(2):
                    card = cards[(i*2)+j]
                    next_gamestate.hands[j].append(card)
                    next_gamestate.deck.remove(card)
            self.calc_br(next_gamestate, br_strat_map, 0, 1.0)
            self.calc_br(next_gamestate, br_strat_map, 1, 1.0)
        for _,v in br_strat_map.items():
            v[:] = np.where(v == np.max(v), 1, 0)
        cfr_strategy = self.trainer.get_final_strategy()
        for cards in perms:
            next_gamestate = copy.deepcopy(gamestate)
            next_gamestate.hands = [[],[]]
            for i in range(gamestate.rules.hand_size):
                for j in range(2):
                    card = cards[i+j]
                    next_gamestate.hands[j].append(card)
                    next_gamestate.deck.remove(card)
            ev_1 = self.calc_ev(next_gamestate, cfr_strategy, br_strat_map)
            ev_2 = self.calc_ev(next_gamestate, br_strat_map, cfr_strategy)
            exploitability += (ev_1 - ev_2)

        trainer.nodes_touched = nodes_touched #keeping this the same as b4
        return exploitability * (1.0/perm_num) , br_strat_map

    def calc_br(self, gamestate, br_strat_map, br_player, prob):
        if gamestate.is_terminal():
            return -self.get_utility(gamestate)
        if gamestate.is_chance():
            return self.chancebr(gamestate, br_strat_map, br_player, prob)
        else:
            return self.decisionbr(gamestate, br_strat_map, br_player, prob)

    def chancebr(self, gamestate, br_strat_map, br_player, prob):
        chance_outcomes = gamestate.get_public_chanceoutcomes()
        chance_prob = 1.0/len(chance_outcomes)
        utility = 0
        for outcome in chance_outcomes:
            next_gamestate = gamestate.handle_public_chance(outcome)
            utility += self.calc_br(next_gamestate, br_strat_map, br_player, prob)
        return utility * chance_prob

    def decisionbr(self, gamestate, br_strat_map, br_player, prob):
        player = gamestate.get_active_player_index()
        key = gamestate.get_representation(gamestate.hands[player])
        possible_actions = gamestate.get_actions()
        n_actions = len(possible_actions)

        vals = np.zeros(n_actions)
        if player == br_player:
            for ia, action in enumerate(possible_actions):
                next_gamestate = gamestate.handle_action(action)
                vals[ia] = self.calc_br(next_gamestate, br_strat_map, br_player, prob)
            br_value = max(vals)
            if key not in br_strat_map:
                br_strat_map[key] = np.zeros(n_actions)
            br_strat_map[key] = br_strat_map[key] + prob * np.array(vals, dtype=np.float64)
            return -br_value
        else:
            strategy = self.trainer.get_node(key, n_actions).get_average_strategy()
            for ia, action in enumerate(possible_actions):
                next_gamestate = gamestate.handle_action(action)
                vals[ia] = self.calc_br(next_gamestate, br_strat_map, br_player, prob * strategy[ia])
            return -np.dot(strategy, vals)

    def calc_ev(self, gamestate, p1_strat, p2_strat):
        if gamestate.is_terminal():
            return -self.get_utility(gamestate)
        if gamestate.is_chance():
            return self.chanceev(gamestate, p1_strat, p2_strat)
        else:
            return self.decisionev(gamestate, p1_strat, p2_strat)

    def chanceev(self, gamestate, p1_strat, p2_strat):
        chance_outcomes = gamestate.get_public_chanceoutcomes()
        chance_prob = 1.0/len(chance_outcomes)
        next_evs  = 0
        for outcome in chance_outcomes:
            next_gamestate = gamestate.handle_public_chance(outcome)
            next_evs += self.calc_ev(next_gamestate, p1_strat, p2_strat)
        return next_evs * chance_prob

    def decisionev(self, gamestate, p1_strat, p2_strat):
        player = gamestate.get_active_player_index()
        key = gamestate.get_representation(gamestate.hands[player])

        if player == 0:
            strat = p1_strat[key]
        else:
            strat = p2_strat[key]

        possible_actions = gamestate.get_actions()
        n_actions = len(possible_actions)

        next_evs = np.zeros(n_actions)
        for ia, action in enumerate(possible_actions):
            next_gamestate = gamestate.handle_action(action)
            next_evs[ia] = self.calc_ev(next_gamestate, p1_strat, p2_strat)
        return -np.dot(strat, next_evs)

    def get_utility(self, gamestate):
        player = gamestate.get_active_player_index() 
        payoff = gamestate.get_payoff()

        if gamestate.is_fold():
            return payoff
            
        player_rank = gamestate.get_player_rank(player)
        opp_rank = gamestate.get_player_rank(1-player)
        if player_rank > opp_rank:
            return payoff
        elif player_rank < opp_rank:
            return -payoff
        else:
            return 0

    
class Exploit_Vec_Calc:
    def compute_exploitability(self, trainer):
        nodes_touched = trainer.nodes_touched
        self.trainer = trainer
        gamestate = copy.deepcopy(self.trainer.gamestate)
        gamestate.vectorised = True
        comb_hands = itertools.combinations(gamestate.deck, gamestate.rules.hand_size)
        enum_hands = list(enumerate(comb_hands))
        gamestate.hands = enum_hands
        gamestate.sort_by_ranking()
        self.private_states = len(gamestate.hands)
        self.fci = np.repeat(1.0/self.private_states, self.private_states)
        exploitability = 0
        br_strat_map = {}
    
        for player in range(2):
            rps_2 = np.repeat(1.0, self.private_states) #opp reach prob
            next_gamestate = copy.deepcopy(gamestate)
            self.calc_br(next_gamestate, br_strat_map, player, rps_2)
        #------------------
        #print(br_strat_map)
        for _,v in br_strat_map.items():
            v[:] = np.where(v == np.max(v), 1, 0)
        print(br_strat_map)
        cfr_strategy = self.trainer.get_final_strategy()
        next_gamestate = copy.deepcopy(gamestate)
        ev_1 = np.sum(self.calc_ev(next_gamestate, cfr_strategy, br_strat_map))
        ev_2 = np.sum(self.calc_ev(next_gamestate, br_strat_map, cfr_strategy))
        exploitability += (1.0/(self.private_states)) * (ev_1 - ev_2)

        trainer.nodes_touched = nodes_touched #keeping this the same as b4
        return exploitability, br_strat_map

    def calc_br(self, gamestate, br_strat_map, br_player, prob):
        if gamestate.is_terminal():
            return self.terminalbr(gamestate, br_player, prob)
        if gamestate.is_chance():
            return self.chancebr(gamestate, br_strat_map, br_player, prob)
        else:
            return self.decisionbr(gamestate, br_strat_map, br_player, prob)

    def terminalbr(self, gamestate, br_player, prob):
        return self.get_brutility(gamestate, br_player, prob)

    def get_brutility(self, gamestate, br_player, rps_2):
        print('hnd: {}'.format(gamestate.hands))
        print('rnk: {}'.format(gamestate.ranks_tuple))
        print('hst: {}'.format(gamestate.history))
        print('rp: {}'.format(rps_2))
        lookuptable = gamestate.rules.lookuptable
        utility = np.zeros(self.private_states)
        payoff = gamestate.get_payoff()
        
        if gamestate.is_fold():
            player = gamestate.get_active_player_index() 
            total_rp = sum(rps_2)
            removecr = np.zeros(len(lookuptable))
            for index, hand in gamestate.hands:
                for card in hand:
                    removecr[lookuptable[card]] += rps_2[index] 
                
            for index, hand in gamestate.hands:
                temp_rp = total_rp
                for card in hand:
                    temp_rp -= removecr[lookuptable[card]]
                utility[index] += temp_rp * payoff

            tmp = utility if player==br_player else -utility
            print('util: {}'.format(tmp))
            return tmp

        wincr = np.zeros(len(lookuptable))
        winsum = 0
        j = 0    
        for index, rank, hand in gamestate.ranks_tuple:
            while gamestate.ranks_tuple[j][1] < rank: #rank of opp hand
                opp_index, _, ophand = gamestate.ranks_tuple[j]
                winsum += rps_2[opp_index]
                for card in ophand:
                    wincr[lookuptable[card]] += rps_2[opp_index] 
                j += 1
            thiswinsum = winsum
            for card in hand:
                thiswinsum -= wincr[lookuptable[card]]
            utility[index] += thiswinsum * payoff

        losecr = np.zeros(len(lookuptable))
        losesum = 0
        j = len(gamestate.ranks_tuple) - 1
        reversed_tuple = copy.deepcopy(gamestate.ranks_tuple)
        reversed_tuple.reverse()
        for index, rank, hand in reversed_tuple:
            while gamestate.ranks_tuple[j][1] > rank:
                opp_index, _, ophand = gamestate.ranks_tuple[j]
                losesum += rps_2[opp_index]
                for card in ophand:
                    losecr[lookuptable[card]] += rps_2[opp_index] 
                j -= 1
            thislosesum = losesum
            for card in hand:
                thislosesum -= losecr[lookuptable[card]]
            utility[index] -= thislosesum * payoff
        print('util: {}'.format(utility))
        return utility

    def chancebr(self, gamestate, br_strat_map, br_player, prob):
        chance_outcomes = gamestate.get_public_chanceoutcomes()
        chance_prob = 1.0/len(chance_outcomes)
        utility  = np.zeros(self.private_states)
        for outcome in chance_outcomes:
            next_gamestate = gamestate.handle_public_chance(outcome)
            utility += self.calc_br(next_gamestate, br_strat_map, br_player, prob)
        return utility * chance_prob

    def decisionbr(self, gamestate, br_strat_map, br_player, prob):
        player = gamestate.get_active_player_index()
        possible_actions = gamestate.get_actions()
        n_actions = len(possible_actions)
        
        keys, indicies_grouping = self.trainer.group_hands_by_key(gamestate)
        
        vals = np.zeros((n_actions, self.private_states))
        if player == br_player:
            #keys used here would use hand isomorphisms
            for ia, action in enumerate(possible_actions):
                next_gamestate = gamestate.handle_action(action)
                vals[ia] = self.calc_br(next_gamestate, br_strat_map, br_player, prob)
            #print('1: {}'.format(vals))
            br_values = np.array([max(vals[:,iI]) for iI in range(self.private_states)])
            #print('1br: {}'.format(br_values))
            for iI, indicies in enumerate(indicies_grouping):
                key = keys[iI]
                if key not in br_strat_map:
                    br_strat_map[key] = np.zeros(n_actions)
                for index in indicies:
                    br_strat_map[key] += prob[iI] * np.array(vals[:,iI], dtype=np.float64)
            return br_values

        else:
            #keys used here use the relavent hand abstraction from training
            f = lambda key: self.trainer.node_map[key].get_average_strategy()
            temp_strategy_vec = list(map(f, keys)) 
            strategy_vec = np.zeros((self.private_states, n_actions))

            for iI, indicies in enumerate(indicies_grouping):
                strategy = temp_strategy_vec[iI]
                for index in indicies:
                    strategy_vec[index] = strategy

            for ia, action in enumerate(possible_actions):
                next_gamestate = gamestate.handle_action(action)
                vals[ia] = self.calc_br(next_gamestate, br_strat_map, br_player, prob * strategy_vec[:,ia]) 
            #print('2: {}'.format(vals))
            finds = np.zeros(self.private_states)
            for iI, strat in enumerate(strategy_vec):
                finds[iI] = np.dot(strat, vals[:,iI])
            return finds

    def calc_ev(self, gamestate, p1_strat, p2_strat):
        if gamestate.is_terminal():
            return -self.terminalev(gamestate)
        if gamestate.is_chance():
            return self.chanceev(gamestate, p1_strat, p2_strat)
        else:
            return self.decisionev(gamestate, p1_strat, p2_strat)

    def terminalev(self, gamestate):
        lookuptable = gamestate.rules.lookuptable
        utility = np.zeros(self.private_states)
        player = gamestate.get_active_player_index() 
        payoff = gamestate.get_payoff()

        totalmatches = len(gamestate.hands)
        seencr = np.zeros(len(lookuptable))
        for index, hand in gamestate.hands:
            for card in hand:
                seencr[lookuptable[card]] += 1

        if gamestate.is_fold():     
            for index, hand in gamestate.hands:
                thistotalwins = totalmatches
                for card in hand:
                    thistotalwins -= seencr[lookuptable[card]]
                utility[index] += thistotalwins
            return (utility * payoff) / totalmatches

        winsum = 0
        j = 0    
        for index, rank, hand in gamestate.ranks_tuple:
            while gamestate.ranks_tuple[j][1] < rank: #rank of opp hand
                winsum += 1
                j += 1 
            thiswinsum = winsum
            for card in hand:
                thiswinsum -= seencr[lookuptable[card]]
            utility[index] += thiswinsum

        losesum = 0
        j = len(gamestate.ranks_tuple) - 1
        reversed_tuple = gamestate.ranks_tuple.copy()
        reversed_tuple.reverse()
        for index, rank, hand in reversed_tuple:
            while gamestate.ranks_tuple[j][1] > rank: #rank of opp hand
                losesum += 1
                j -= 1 
            thislosesum = losesum
            for card in hand:
                thislosesum -= seencr[lookuptable[card]]
            utility[index] -= thislosesum

        return (utility * payoff) / totalmatches

    def chanceev(self, gamestate, p1_strat, p2_strat):
        chance_outcomes = gamestate.get_public_chanceoutcomes()
        chance_prob = 1.0/len(chance_outcomes)
        next_evs  = np.zeros(self.private_states)
        for outcome in chance_outcomes:
            next_gamestate = gamestate.handle_public_chance(outcome)
            next_evs += self.calc_ev(next_gamestate, p1_strat, p2_strat)
        return next_evs * chance_prob
    
    def decisionev(self, gamestate, p1_strat, p2_strat):
        player = gamestate.get_active_player_index()
        keys, indicies_grouping = self.trainer.group_hands_by_key(gamestate)

        if player == 0:
            f = lambda key: p1_strat[key]
            strategys = list(map(f, keys))
        else:
            f = lambda key: p2_strat[key]
            strategys = list(map(f, keys))

        possible_actions = gamestate.get_actions()
        n_actions = len(possible_actions)

        next_evs = np.zeros((n_actions, self.private_states))
        for ia, action in enumerate(possible_actions):
            next_gamestate = gamestate.handle_action(action)
            next_evs[ia] = self.calc_ev(next_gamestate, p1_strat, p2_strat)

        this_evs = np.zeros(self.private_states)
        for iI, indicies in enumerate(indicies_grouping):
            strat = strategys[iI]
            evs = next_evs[:,iI]
            for index in indicies:
                this_evs[index] = np.dot(strat, evs)
        return -this_evs