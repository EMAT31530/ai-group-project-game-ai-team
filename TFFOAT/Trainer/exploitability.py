import numpy as np
import itertools
import copy

class Exploit_Calc:
    def __init__(self):
        self.trainer = 0


    def compute_exploitability(self, trainer):
        self.trainer = trainer
        gamestate = copy.deepcopy(self.trainer.gamestate)
        gamestate.history = 'dd'
        exploitability = 0
        br_strat_map = {}

        perms = itertools.permutations(gamestate.deck, 2)
        lent = 0
        for cards in perms:
            lent += 1
            next_gamestate = copy.deepcopy(gamestate)
            next_gamestate.cards = cards
            self.calc_br(next_gamestate, br_strat_map, 0, 1.0)
            self.calc_br(next_gamestate, br_strat_map, 1, 1.0)
        for _,v in br_strat_map.items():
            v[:] = np.where(v == np.max(v), 1, 0)
        cfr_strategy = self.trainer.get_final_strategy()

        
        for cards in itertools.permutations(gamestate.deck):
            next_gamestate = copy.deepcopy(gamestate)
            next_gamestate.cards = cards
            ev_1 = self.calc_ev(next_gamestate, cfr_strategy, br_strat_map)
            ev_2 = self.calc_ev(next_gamestate, br_strat_map, cfr_strategy)
            exploitability += (1.0/lent) * (ev_1 - ev_2)
        return exploitability, br_strat_map


    def calc_br(self, gamestate, br_strat_map, br_player, prob):
        if gamestate.is_terminal():
            return -gamestate.get_rewards()
        key = gamestate.get_representation()
        player = gamestate.get_active_player_index()
        possible_actions = gamestate.get_actions()
        n_actions = len(possible_actions)

        vals = np.zeros(n_actions)
        if player == br_player:
            for ia, action in enumerate(possible_actions):
                next_gamestate = gamestate.handle_action(action)
                vals[ia] = self.calc_br(next_gamestate, br_strat_map, br_player, prob)

            br_value = max(vals)
            if key not in br_strat_map:
                br_strat_map[key] = np.array([0.0, 0.0])
            br_strat_map[key] = br_strat_map[key] + prob * np.array(vals, dtype=np.float64)
            return -br_value
        else:
            strategy = self.trainer.get_node(key, 0).get_average_strategy()
            for ia, action in enumerate(possible_actions):
                next_gamestate = gamestate.handle_action(action)
                vals[ia] = self.calc_br(next_gamestate, br_strat_map, br_player, prob * strategy[ia])
            return -np.dot(strategy, vals)

    def calc_ev(self, gamestate, p1_strat, p2_strat):
        if gamestate.is_terminal():
            return -gamestate.get_rewards()
        
        player = gamestate.get_active_player_index()
        key = gamestate.get_representation()
  
        if player == 0:
            strat = p1_strat[key]
        else:
            strat = p2_strat[key]

        possible_actions = gamestate.get_actions()
        n_actions = len(possible_actions)

        next_evs = np.zeros(n_actions)
        for ia, action in enumerate(possible_actions):
            next_gamestate = gamestate.handle_action(action)
            next_evs[ia] = self.calc_ev(next_gamestate, p1_strat, p2_strat)
        return -np.dot(strat, next_evs)



